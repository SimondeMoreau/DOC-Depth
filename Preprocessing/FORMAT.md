# File Format for the Pipeline

If you want to use your own SLAM or dynamic object classification method, the output must follow the format described in this document.

## Trajectory (SLAM)

For DOC-Depth, loop closure is not required. Local consistency from a LiDAR odometry method is sufficient for scene reconstruction. You may use any low-drift LiDAR odometry system.

The trajectory should be stored as a **binary PLY file** (`PLY_LITTLE_ENDIAN`) with the following fields:

| Field name | Type   | Description                                   |
|------------|--------|-----------------------------------------------|
| x          | double | X component of the pose’s translation         |
| y          | double | Y component of the pose’s translation         |
| z          | double | Z component of the pose’s translation         |
| qx         | double | X component of the rotation quaternion        |
| qy         | double | Y component of the rotation quaternion        |
| qz         | double | Z component of the rotation quaternion        |
| qw         | double | W (omega) component of the rotation quaternion|
| timestamp  | double | Hardware timestamp of the corresponding LiDAR frame |
| indices    | int    | Index of the frame in the dataset             |

## LiDAR Frames (Classification)

If you're using a custom classifier, each LiDAR frame should be stored as a PLY file with **at least** the following fields:

| Field name | Type    | Description                    |
|------------|---------|--------------------------------|
| x          | float   | X coordinate of the point      |
| y          | float   | Y coordinate of the point      |
| z          | float   | Z coordinate of the point      |
| classid    | u_short | Class ID of the point          |

**Class ID ranges:**
- `classid < 50`: Ground points  
- `50 ≤ classid < 100`: Static objects  
- `classid ≥ 100`: Dynamic objects  

## Folder Structure

Your directory should be organized as follows:
```
run/
├── traj_odometry.ply
├── traj_camera.ply
├── calib.json
├── calib_lidar_ref_cam.txt
└── frames/
    ├── frame_000000.ply
    ├── frame_000001.ply
    └── ...

```

## Calibration Files
### `calib.json`

Contains camera intrinsics:

```json
{
  "mtx": [[K1, K2, K3], [K4, K5, K6], [K7, K8, K9]]
}
```

### `calib_lidar_ref_cam.txt` 
4x4 transform matrix of the position of the lidar in the camera reference: 
```
R1 R2 R3 T1
R4 R5 R6 T2
R7 R8 R9 T3
0  0  0  1
```

## Camera trajectory
`traj_camera.ply` should be generated by interpolating `traj_odometry.ply` at the image timestamps.

You can refer to the script [camera_trajectory.py](camera_trajectory.py) for implementation guidance.


## Rendering
Once all files are prepared, run the renderer with:

```
python render.py [traj_odometry.ply file] [traj_camera.ply file] [lidar_frame_directory] --intrinsic [calib.json file] --extrinsic [calib_lidar_ref_cam.txt file] -o [output_directory]
```